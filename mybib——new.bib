@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proc. Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol.},
  year={2019},
}

@inproceedings{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  booktitle = {Proc. Annu. Meet. Assoc. Comput. Linguist.},
  year      = {2019},
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Proc. Adv. neural inf. proces. syst.},
  year={2020},
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Proc. Adv. neural inf. proces. syst.},
  pages={5998--6008},
  year={2017}
}

@inproceedings{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
  booktitle={Proc. Adv. neural inf. proces. syst.},
  year={2019},
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={Proc. Int. Conf. Learn. Representations},
  year={2021}
}


@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={Proc. Int. Conf. Learn. Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={Proc. Eur. Conf. Comput. Vis.},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{beal2020toward,
  title={Toward transformer-based object detection},
  author={Beal, Josh and Kim, Eric and Tzeng, Eric and Park, Dong Huk and Zhai, Andrew and Kislyuk, Dmitry},
  journal={arXiv preprint arXiv:2012.09958},
  year={2020}
}

@article{strudel2021segmenter,
  title={Segmenter: Transformer for Semantic Segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  journal={arXiv preprint arXiv:2105.05633},
  year={2021}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proc. IEEE Int. Conf. Computer Vis. Pattern Recognit.},
  pages={6881--6890},
  year={2021}
}

@article{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:2104.14294},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={Proc. Int. Conf. Mach. Learn.},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ramesh2021zero,
  title={Zero-Shot Text-to-Image Generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}


@inproceedings{bhojanapalli2021understanding,
  title={Understanding robustness of transformers for image classification},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={10231--10241},
  year={2021}
}

@article{shao2021adversarial,
  title={On the Adversarial Robustness of Vision Transformers},
  author={Shao, Rulin and Shi, Zhouxing and Yi, Jinfeng and Chen, Pin-Yu and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2103.15670},
  year={2021}
}

@article{aldahdooh2021reveal,
  title={Reveal of Vision Transformers Robustness against Adversarial Attacks},
  author={Aldahdooh, Ahmed and Hamidouche, Wassim and Deforges, Olivier},
  journal={arXiv preprint arXiv:2106.03734},
  year={2021}
}

@article{mao2021rethinking,
  title={Rethinking the Design Principles of Robust Vision Transformer},
  author={Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Ye, Shaokai and He, Yuan and Xue, Hui},
  journal={arXiv preprint arXiv:2105.07926},
  year={2021}
}

@inproceedings{fu2022patchfool,
title={Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?},
author={Yonggan Fu and Shunyao Zhang and Shang Wu and Cheng Wan and Yingyan Lin},
booktitle={Proc. Int. Conf. Learn. Representations},
year={2022},
url={https://openreview.net/forum?id=28ib9tf6zhr}
}

@article{bai2021transformers,
  title={Are Transformers more robust than CNNs?},
  author={Bai, Yutong and Mei, Jieru and Yuille, Alan L and Xie, Cihang},
  journal={Proc. Adv. neural inf. proces. syst.},
  volume={34},
  year={2021}
}

@article{shrikumar2016not,
  title={Not just a black box: Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Shcherbina, Anna and Kundaje, Anshul},
  journal={arXiv preprint arXiv:1605.01713},
  year={2016}
}

@article{smilkov2017smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017}
}

@inproceedings{srinivas2019full,
  title={Full-gradient representation for neural network visualization},
  author={Srinivas, Suraj and Fleuret, Fran{\c{c}}ois},
  booktitle={Proc. Adv. neural inf. proces. syst.},
  pages={4126--4135},
  year={2019}
}

@inproceedings{li2018tell,
  title={Tell me where to look: Guided attention inference network},
  author={Li, Kunpeng and Wu, Ziyan and Peng, Kuan-Chuan and Ernst, Jan and Fu, Yun},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit.},
  pages={9215--9223},
  year={2018}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={618--626},
  year={2017}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={Proc. Int. Conf. Mach. Learn.},
  pages={3145--3153},
  year={2017},
  organization={JMLR. org}
}

@article{jiang2021layercam,
  title={Layercam: Exploring hierarchical class activation maps for localization},
  author={Jiang, Peng-Tao and Zhang, Chang-Bin and Hou, Qibin and Cheng, Ming-Ming and Wei, Yunchao},
  journal={IEEE Trans. Image Process.},
  volume={30},
  pages={5875--5888},
  year={2021},
  publisher={IEEE}
}

@inproceedings{muhammad2020eigen,
  title={Eigen-cam: Class activation map using principal components},
  author={Muhammad, Mohammed Bany and Yeasin, Mohammed},
  booktitle={Proc. Int. Jt. Conf. Neural Networks},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@inproceedings{wang2020score,
  title={Score-CAM: Score-weighted visual explanations for convolutional neural networks},
  author={Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
  booktitle={Proc. IEEE Int. Conf. Computer Vis. Pattern Recognit. Workshops},
  pages={24--25},
  year={2020}
}

@inproceedings{ramaswamy2020ablation,
  title={Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization},
  author={Ramaswamy, Harish Guruprasad and others},
  booktitle={Proc. IEEE Winter Conf. Appl. Comput. Vis.},
  pages={983--991},
  year={2020}
}

@article{fu2020axiom,
  title={Axiom-based grad-cam: Towards accurate visualization and explanation of cnns},
  author={Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
  journal={arXiv preprint arXiv:2008.02312},
  year={2020}
}

@inproceedings{chattopadhay2018grad,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={Proc. IEEE Winter Conf. Appl. Comput. Vis.},
  pages={839--847},
  year={2018},
  organization={IEEE}
}

@misc{jacobgilpytorchcam,
  title={PyTorch library for CAM methods},
  author={Jacob Gildenblat and contributors},
  year={2021},
  publisher={GitHub},
  howpublished={\url{https://github.com/jacobgil/pytorch-grad-cam}},
}

@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proc. IEEE Int. Conf. Computer Vis. Pattern Recognit.},
  pages={782--791},
  year={2021}
}

@inproceedings{chefer2021generic,
  title={Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={397--406},
  year={2021}
}


@article{rauber2017foolboxnative,
  doi = {10.21105/joss.02607},
  url = {https://doi.org/10.21105/joss.02607},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {53},
  pages = {2607},
  author = {Jonas Rauber and Roland Zimmermann and Matthias Bethge and Wieland Brendel},
  title = {Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX},
  journal = {J. Open Source Softw.}
}

@inproceedings{rauber2017foolbox,
  title={Foolbox: A Python toolbox to benchmark the robustness of machine learning models},
  author={Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},
  booktitle={Proc. Int. Conf. Mach. Learn. Workshop},
  year={2017},
  url={http://arxiv.org/abs/1707.04131},
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@article{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  journal={Proc. Adv. neural inf. proces. syst.},
  volume={32},
  year={2019}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={Proc. Int. Conf. Learn. Representations},
  year={2015}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={Proc. Int. Conf. Learn. Representations},
  year={2018}
}

@inproceedings{moosavi2016deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit.},
  pages={2574--2582},
  year={2016}
}

@inproceedings{rony2019decoupling,
  title={Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses},
  author={Rony, J{\'e}r{\^o}me and Hafemann, Luiz G and Oliveira, Luiz S and Ayed, Ismail Ben and Sabourin, Robert and Granger, Eric},
  booktitle={Proc. IEEE Int. Conf. Computer Vis. Pattern Recognit.},
  pages={4322--4330},
  year={2019}
}

@article{DBLP:journals/tdsc/LiangLSLSW21,
  author    = {Bin Liang and
               Hongcheng Li and
               Miaoqiang Su and
               Xirong Li and
               Wenchang Shi and
               Xiaofeng Wang},
  title     = {Detecting Adversarial Image Examples in Deep Neural Networks with
               Adaptive Noise Reduction},
  journal   = {{IEEE} Trans. Dependable Secur. Comput.},
  volume    = {18},
  number    = {1},
  pages     = {72--85},
  year      = {2021},
  url       = {https://doi.org/10.1109/TDSC.2018.2874243},
  doi       = {10.1109/TDSC.2018.2874243},
  timestamp = {Sat, 09 Oct 2021 13:18:02 +0200},
  biburl    = {https://dblp.org/rec/journals/tdsc/LiangLSLSW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={10012--10022},
  year={2021}
}

@inproceedings{meng2017magnet,
  title={Magnet: a two-pronged defense against adversarial examples},
  author={Meng, Dongyu and Chen, Hao},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={135--147},
  year={2017}
}

@inproceedings{li2021patch,
  title={Patch Vestiges in the Adversarial Examples Against Vision Transformer Can Be Leveraged for Adversarial Detection},
  author={Li, Juzheng},
  booktitle={The AAAI-22 Workshop on Adversarial Machine Learning and Beyond},
  year={2021}
}

@software{Howard_Imagenette_2019,
    title={Imagenette: A smaller subset of 10 easily classified classes from Imagenet},
    author={Jeremy Howard},
    year={2019},
    month={March},
    publisher = {GitHub},
    url = {https://github.com/fastai/imagenette}
}

@article{ding2022consensus,
  title={Consensus Adversarial Defense Method Based on Augmented Examples},
  author={Ding, Xintao and Cheng, Yongqiang and Luo, Yonglong and Li, Qingde and Gope, Prosanta},
  journal={IEEE Trans. Ind. Inform.},
  volume={19},
  number={1},
  pages={984--994},
  year={2022},
  publisher={IEEE}
}

@article{xu2020adversarial,
  title={Adversarial attack against urban scene segmentation for autonomous vehicles},
  author={Xu, Xing and Zhang, Jingran and Li, Yujie and Wang, Yichuan and Yang, Yang and Shen, Heng Tao},
  journal={IEEE Trans. Ind. Inform.},
  volume={17},
  number={6},
  pages={4117--4126},
  year={2020},
  publisher={IEEE}
}

@article{zhu2020dual,
  title={Dual-domain-based adversarial defense with conditional VAE and Bayesian network},
  author={Zhu, Jinlin and Peng, Guohao and Wang, Danwei},
  journal={IEEE Trans. Ind. Inform.},
  volume={17},
  number={1},
  pages={596--605},
  year={2020},
  publisher={IEEE}
}

@inproceedings{mahmood2021robustness,
  title={On the robustness of vision transformers to adversarial examples},
  author={Mahmood, Kaleel and Mahmood, Rigel and Van Dijk, Marten},
  booktitle={Proc. IEEE Int. Conf. Comput. Vis.},
  pages={7838--7847},
  year={2021}
}